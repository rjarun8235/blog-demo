{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SearchEvaluation:\n",
    "    \"\"\"\n",
    "    Represents a single search evaluation.\n",
    "    \n",
    "    predicted: Ordered list of items our search system returned\n",
    "    validation: Set of items that are actually relevant\n",
    "    \"\"\"\n",
    "    predicted: List[int]  \n",
    "    validation: Set[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(eval: SearchEvaluation) -> float:\n",
    "    \"\"\"\n",
    "    Calculate basic precision for a single search.\n",
    "    \n",
    "    Real-world example:\n",
    "    predicted = [PyCharm, VSCode, Sublime, Atom, Eclipse]\n",
    "    validation = {PyCharm, VSCode, Jupyter, Spyder}\n",
    "    Result: 2/5 = 0.4 (40% precision)\n",
    "    \"\"\"\n",
    "    if not eval.predicted:\n",
    "        return 0.0\n",
    "    \n",
    "    retrieved_set = set(eval.predicted)\n",
    "    relevant_retrieved = len(eval.validation.intersection(retrieved_set))\n",
    "    return relevant_retrieved / len(retrieved_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(eval: SearchEvaluation, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate precision for top-k results.\n",
    "    \n",
    "    Real-world example:\n",
    "    For k=3:\n",
    "    predicted = [PyCharm, VSCode, Sublime, Atom, Eclipse]\n",
    "    validation = {PyCharm, VSCode, Jupyter, Spyder}\n",
    "    Only look at [PyCharm, VSCode, Sublime]\n",
    "    Result: 2/3 ≈ 0.67 (67% precision at k=3)\n",
    "    \"\"\"\n",
    "    if k <= 0 or not eval.predicted:\n",
    "        return 0.0\n",
    "    \n",
    "    top_k = eval.predicted[:k]\n",
    "    return precision(SearchEvaluation(top_k, eval.validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_at_k(eval: SearchEvaluation, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate position-aware precision up to position k.\n",
    "    \n",
    "    Real-world example:\n",
    "    predicted = [PyCharm, Sublime, VSCode]  (k=3)\n",
    "    validation = {PyCharm, VSCode}\n",
    "    \n",
    "    Let's break it down:\n",
    "    Position 1 (PyCharm): 1/1 = 1.0 (found a relevant item)\n",
    "    Position 2 (Sublime): No change (not relevant)\n",
    "    Position 3 (VSCode): 2/3 ≈ 0.67 (found second relevant item)\n",
    "    \n",
    "    AP@3 = (1.0 + 0.67) / 2 ≈ 0.835\n",
    "    \"\"\"\n",
    "    if k <= 0 or not eval.predicted:\n",
    "        return 0.0\n",
    "    \n",
    "    precisions = []\n",
    "    num_relevant = 0\n",
    "    \n",
    "    for i in range(min(k, len(eval.predicted))):\n",
    "        if eval.predicted[i] in eval.validation:\n",
    "            num_relevant += 1\n",
    "            precisions.append(num_relevant / (i + 1))\n",
    "    \n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "        \n",
    "    return sum(precisions) / min(k, len(eval.validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(evaluations: List[SearchEvaluation], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate overall system performance across multiple searches.\n",
    "    \n",
    "    Example scenario:\n",
    "    Search 1: \"python ide\" → AP@3 = 0.835\n",
    "    Search 2: \"python web frameworks\" → AP@3 = 0.92\n",
    "    Search 3: \"python data science\" → AP@3 = 0.76\n",
    "    \n",
    "    MAP@3 = (0.835 + 0.92 + 0.76) / 3 ≈ 0.838\n",
    "    \"\"\"\n",
    "    if not evaluations:\n",
    "        return 0.0\n",
    "    \n",
    "    ap_scores = [ap_at_k(eval, k) for eval in evaluations]\n",
    "    return sum(ap_scores) / len(evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample searches\n",
    "searches = [\n",
    "    # Search: \"python ide\"\n",
    "    SearchEvaluation(\n",
    "        predicted=[1, 2, 3, 4, 5],  # 1=PyCharm, 2=VSCode, etc.\n",
    "        validation={1, 3, 5}        # PyCharm, Sublime, Eclipse are relevant\n",
    "    ),\n",
    "    # Search: \"python web frameworks\"\n",
    "    SearchEvaluation(\n",
    "        predicted=[2, 4, 1, 3, 5],  # 2=Django, 4=Flask, etc.\n",
    "        validation={1, 2, 3}        # Django, Flask, FastAPI are relevant\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search 1:\n",
      "  Precision@3: 0.667\n",
      "  AP@3: 0.556\n",
      "Search 2:\n",
      "  Precision@3: 0.667\n",
      "  AP@3: 0.556\n",
      "\n",
      "Overall System MAP@3: 0.556\n"
     ]
    }
   ],
   "source": [
    "k = 3  # We care about top 3 results\n",
    "# Evaluate individual searches\n",
    "for i, search in enumerate(searches, 1):\n",
    "    print(f\"Search {i}:\")\n",
    "    print(f\"  Precision@{k}: {precision_at_k(search, k):.3f}\")\n",
    "    print(f\"  AP@{k}: {ap_at_k(search, k):.3f}\")\n",
    "# Evaluate overall system\n",
    "print(f\"\\nOverall System MAP@{k}: {map_at_k(searches, k):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
